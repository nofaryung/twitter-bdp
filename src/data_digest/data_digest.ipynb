{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tables in the database:\n",
      "tweets_by_likes\n",
      "tweets_likes_low\n",
      "tweets_by_share\n",
      "tweets_likes_medium\n",
      "tweets_likes_high\n",
      "tweets_likes_very_high\n",
      "tweets_shares_low\n",
      "tweets_shares_medium\n",
      "tweets_shares_high\n",
      "tweets_shares_very_high\n",
      "user_tweets\n",
      "user_tweets_part1\n",
      "user_tweets_part2\n",
      "user_tweets_part3\n",
      "user_tweets_part4\n",
      "tweets_by_word\n",
      "tweets_by_date_2009\n",
      "tweets_by_date_2011\n",
      "tweets_by_date_2010\n",
      "tweets_by_date_2012\n",
      "tweets_by_date_2013\n",
      "tweets_by_date_2014\n",
      "tweets_by_date_2015\n",
      "tweets_by_date_2016\n",
      "tweets_by_date_2017\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Establish a connection to the database\n",
    "# Adjust the connection parameters as necessary\n",
    "#conn = psycopg2.connect(\n",
    "#    dbname=\"twitty\",\n",
    "#    user=\"postgresuser\",\n",
    "#    password=\"postgrespassword\",\n",
    "#    host=\"localhost\",\n",
    "#    port=\"5432\"\n",
    "#)\n",
    "\n",
    "\n",
    "# Connection parameters - replace these with your actual parameters\n",
    "dbname = 'twitty'\n",
    "user = 'postgresuser'\n",
    "password = 'postgrespassword'\n",
    "host = 'localhost'  # Use 'localhost' if you're port-forwarding the service to your local machine\n",
    "port = '5432'  # Default PostgreSQL port or the NodePort if exposed directly\n",
    "\n",
    "# SQL query to list tables in the database\n",
    "sql_query = \"\"\"\n",
    "SELECT table_name\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = 'public'\n",
    "\"\"\"\n",
    " # Connect to the database\n",
    "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)\n",
    "\n",
    "    # Create a cursor for executing queries\n",
    "cur = conn.cursor()\n",
    "\n",
    "    # Execute the SQL query\n",
    "cur.execute(sql_query)\n",
    "\n",
    "    # Fetch the result (list of tables in the database)\n",
    "tables = cur.fetchall()\n",
    "print(\"List of tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "    # Close the cursor and the connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author | Tweet Count\n",
      "-------------------\n",
      "rihanna | 2877\n",
      "Cristiano | 2507\n",
      "jimmyfallon | 3123\n",
      "jtimberlake | 2478\n",
      "KimKardashian | 2939\n",
      "britneyspears | 2776\n",
      "instagram | 2577\n",
      "taylorswift13 | 2029\n",
      "katyperry | 2924\n",
      "YouTube | 3077\n",
      "selenagomez | 2913\n",
      "cnnbrk | 1842\n",
      "shakira | 2530\n",
      "ddlovato | 2217\n",
      "BarackObama | 2863\n",
      "ArianaGrande | 3104\n",
      "TheEllenShow | 3147\n",
      "ladygaga | 2329\n",
      "Twitter | 2290\n",
      "justinbieber | 2000\n"
     ]
    }
   ],
   "source": [
    "dbname = 'twitty'  # The database name\n",
    "user = 'postgresuser'  # The database user\n",
    "password = 'postgrespassword'  # The user's password\n",
    "host = 'localhost'  # Host address of the PostgreSQL server\n",
    "port = '5432'  # Port number\n",
    "\n",
    "# The SQL query you want to execute\n",
    "sql_query = \"SELECT author, COUNT(tweet_id) as tweet_count FROM user_tweets GROUP BY author;\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)\n",
    "    # Create a cursor object\n",
    "cur = conn.cursor()\n",
    "    # Execute the query\n",
    "cur.execute(sql_query)\n",
    "    # Fetch all rows from the query result\n",
    "rows = cur.fetchall()\n",
    "    # Process the result\n",
    "print(\"Author | Tweet Count\")\n",
    "print(\"-------------------\")\n",
    "for row in rows:\n",
    "    print(f\"{row[0]} | {row[1]}\")\n",
    "    # Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_97k4h5q73m/croot/numpy_and_numpy_base_1704311708477/work/dist/numpy-1.26.3-cp310-cp310-macosx_10_9_x86_64.whl#sha256=9d4230f09aa6cd18b645548b55fa74cb611df850cd99d39a1b636402e018c785\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.read_csv('./tweets_df.csv', delimiter='\t')\n",
    "# tweets_paritioned_by_word = pd.read_csv('/Users/nofary/IDC/BDP/twitter-bdp/src/data_digest/tweets_paritioned_by_word.csv', delimiter='\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df.replace({np.nan: None})\n",
    "twitter_df.drop(columns='Unnamed: 0',inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_paritioned_by_word = twitter_df.explode('parsed_content')\n",
    "tweets_paritioned_by_word = tweets_paritioned_by_word.dropna(subset=[\"parsed_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_to_list(row):\n",
    "    try:\n",
    "        return ast.literal_eval(row)\n",
    "    except ValueError:\n",
    "        return []\n",
    "\n",
    "\n",
    "twitter_df['parsed_content'] = twitter_df['parsed_content'].apply(convert_to_list)\n",
    "\n",
    "raw_twitter_df = list(twitter_df[['tweet_id', 'author', 'content', 'country', 'date_time', 'language', 'latitude', 'longitude', 'number_of_likes', 'number_of_shares', 'parsed_content']].itertuples(index=False, name=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tweets_by_likes\n",
    "insert_tweets_by_likes = \"\"\"INSERT INTO tweets_by_likes (tweet_id, author, content, country, date_time, language, latitude, longitude, number_of_likes, number_of_shares, parsed_content) VALUES %s;\"\"\"\n",
    "execute_values(cur, insert_tweets_by_likes, raw_twitter_df)\n",
    "\n",
    "# tweets_by_share\n",
    "insert_tweets_by_share = \"\"\"INSERT INTO tweets_by_share (tweet_id, author, content, country, date_time, language, latitude, longitude, number_of_likes, number_of_shares, parsed_content) VALUES %s;\"\"\"\n",
    "execute_values(cur, insert_tweets_by_share, raw_twitter_df)\n",
    "\n",
    "# user_tweets\n",
    "insert_user_tweets = \"\"\"INSERT INTO user_tweets (tweet_id, author, content, country, date_time, language, latitude, longitude, number_of_likes, number_of_shares, parsed_content) VALUES %s;\"\"\"\n",
    "execute_values(cur, insert_user_tweets, raw_twitter_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets_paritioned_by_word = list(tweets_paritioned_by_word[['tweet_id', 'author', 'content', 'country', 'date_time', 'language', 'latitude', 'longitude', 'number_of_likes', 'number_of_shares', 'parsed_content']].itertuples(index=False, name=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_tweets\n",
    "insert_tweets_by_word = \"\"\"INSERT INTO tweets_by_word (tweet_id, author, content, country, date_time, language, latitude, longitude, number_of_likes, number_of_shares, parsed_content) VALUES %s;\"\"\"\n",
    "execute_values(cur, insert_tweets_by_word, raw_tweets_paritioned_by_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # SQL command to delete all data from the table\n",
    "    delete_query = \"DELETE FROM tweets_by_likes;\"\n",
    "\n",
    "    # Execute the SQL command\n",
    "    cur.execute(delete_query)\n",
    "\n",
    "    # conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_list(row):\n",
    "    try:\n",
    "        return ast.literal_eval(row)\n",
    "    except ValueError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digest_data():\n",
    "    conn = None\n",
    "    cur = None\n",
    "    try:\n",
    "        # Database connection parameters from environment variables\n",
    "        # dbname = os.getenv(\"POSTGRES_DB\", \"twitty\")\n",
    "        user = os.getenv(\"POSTGRES_USER\", \"postgresuser\")\n",
    "        password = os.getenv(\"POSTGRES_PASSWORD\", \"postgrespassword\")\n",
    "        host = os.getenv(\"DB_URL\", \"localhost\")\n",
    "        port = os.getenv(\"DB_PORT\", \"5432\")\n",
    "\n",
    "        conn = psycopg2.connect(\n",
    "            dbname='twitty',\n",
    "            user=user,\n",
    "            password=password,\n",
    "            host=host\n",
    "            # port=port\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        twitter_df = pd.read_csv('/Users/nofary/IDC/BDP/twitter-bdp/src/data_digest/tweets_df.csv', delimiter='\t')\n",
    "        twitter_df = twitter_df.replace({np.nan: None})\n",
    "\n",
    "        tweets_paritioned_by_word = twitter_df.explode('parsed_content')\n",
    "        tweets_paritioned_by_word = tweets_paritioned_by_word.dropna(subset=[\"parsed_content\"])\n",
    "\n",
    "        twitter_df['parsed_content'] = twitter_df['parsed_content'].apply(convert_to_list)\n",
    "\n",
    "        raw_twitter_df = list(twitter_df[['tweet_id', 'author', 'content', 'country', 'date_time', 'language', 'latitude', 'longitude', 'number_of_likes', 'number_of_shares', 'parsed_content']].itertuples(index=False, name=None))\n",
    "\n",
    "        # tweets_by_likes\n",
    "        insert_tweets_by_likes = \"\"\"INSERT INTO tweets_by_likes (tweet_id, author, content, country, date_time, language, latitude, longitude, number_of_likes, number_of_shares, parsed_content) VALUES %s;\"\"\"\n",
    "        execute_values(cur, insert_tweets_by_likes, raw_twitter_df)\n",
    "\n",
    "        # tweets_by_share\n",
    "        insert_tweets_by_share = \"\"\"INSERT INTO tweets_by_share (tweet_id, author, content, country, date_time, language, latitude, longitude, number_of_likes, number_of_shares, parsed_content) VALUES %s;\"\"\"\n",
    "        execute_values(cur, insert_tweets_by_share, raw_twitter_df)\n",
    "\n",
    "        # user_tweets\n",
    "        insert_user_tweets = \"\"\"INSERT INTO user_tweets (tweet_id, author, content, country, date_time, language, latitude, longitude, number_of_likes, number_of_shares, parsed_content) VALUES %s;\"\"\"\n",
    "        execute_values(cur, insert_user_tweets, raw_twitter_df)\n",
    "\n",
    "        # user_tweets\n",
    "        raw_tweets_paritioned_by_word = list(tweets_paritioned_by_word[['tweet_id', 'author', 'content', 'country', 'date_time', 'language', 'latitude', 'longitude', 'number_of_likes', 'number_of_shares', 'parsed_content']].itertuples(index=False, name=None))\n",
    "        insert_tweets_by_word = \"\"\"INSERT INTO tweets_by_word (tweet_id, author, content, country, date_time, language, latitude, longitude, number_of_likes, number_of_shares, parsed_content) VALUES %s;\"\"\"\n",
    "        execute_values(cur, insert_tweets_by_word, raw_tweets_paritioned_by_word)\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction in case of error\n",
    "        if conn is not None:\n",
    "            conn.rollback()\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Optionally, re-raise the exception if you want the error to propagate\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "        if conn is not None:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
